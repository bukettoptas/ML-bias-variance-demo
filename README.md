# ğŸ“Š Linear Regresyon: SÄ±fÄ±rdan Ä°leri Seviye

> **Makine Ã–ÄŸrenmesi'nin Temel TaÅŸÄ±: Linear Regresyon Ders NotlarÄ±**
> 
> *HazÄ±rlayan: Dr. Buket ToptaÅŸ*  
> *Tarih: 2025*

---

## ğŸ“š Ä°Ã§indekiler

1. [GiriÅŸ](#-giriÅŸ)
2. [Linear Regresyon Nedir?](#-linear-regresyon-nedir)
3. [Matematiksel Temel](#-matematiksel-temel)
4. [Kod Ã–rnekleri](#-kod-Ã¶rnekleri)
5. [GÃ¶rselleÅŸtirme](#-gÃ¶rselleÅŸtirme)
6. [Performans Metrikleri](#-performans-metrikleri)
7. [GerÃ§ek DÃ¼nya UygulamalarÄ±](#-gerÃ§ek-dÃ¼nya-uygulamalarÄ±)
8. [Ä°leri Seviye Konular](#-iÌ‡leri-seviye-konular)
9. [Kaynaklar](#-kaynaklar)

---

## ğŸ¯ GiriÅŸ

Linear Regresyon, **makine Ã¶ÄŸrenmesinin alfabesi**dir. Ä°ki deÄŸiÅŸken arasÄ±ndaki doÄŸrusal iliÅŸkiyi modelleyen bu algoritma, karmaÅŸÄ±k modellerin temelini oluÅŸturur.

### ğŸ“ Bu Derste Ã–ÄŸrenecekleriniz

- âœ… Linear regresyonun matematiksel temelleri
- âœ… Python ve Sklearn ile uygulama
- âœ… Model performansÄ±nÄ± deÄŸerlendirme
- âœ… Overfitting ve underfitting kavramlarÄ±
- âœ… GerÃ§ek dÃ¼nya problemlerini Ã§Ã¶zme

### ğŸ“‹ Ã–n KoÅŸullar

```python
# Gerekli kÃ¼tÃ¼phaneler
numpy>=1.21.0
matplotlib>=3.4.0
scikit-learn>=1.0.0
pandas>=1.3.0
seaborn>=0.11.0
```

---

## ğŸ” Linear Regresyon Nedir?

Linear regresyon, **baÄŸÄ±mlÄ± deÄŸiÅŸken (y)** ile bir veya daha fazla **baÄŸÄ±msÄ±z deÄŸiÅŸken (x)** arasÄ±ndaki **doÄŸrusal iliÅŸkiyi** modelleyen istatistiksel bir yÃ¶ntemdir.

### ğŸ“ Basit FormÃ¼l

```
y = mx + b
```

Burada:
- **y**: Tahmin etmek istediÄŸimiz deÄŸer (baÄŸÄ±mlÄ± deÄŸiÅŸken)
- **x**: BildiÄŸimiz deÄŸer (baÄŸÄ±msÄ±z deÄŸiÅŸken)
- **m**: EÄŸim (slope) - x deÄŸiÅŸince y ne kadar deÄŸiÅŸir?
- **b**: Kesim noktasÄ± (intercept) - x=0 olduÄŸunda y'nin deÄŸeri

### ğŸ¯ Ã–rnek Senaryo: Dondurma SatÄ±ÅŸlarÄ±

> Ahmet Amca bir sahil kasabasÄ±nda dondurma satÄ±yor. SÄ±caklÄ±k arttÄ±kÃ§a satÄ±ÅŸlarÄ± da artÄ±yor. YarÄ±n hava 35Â°C olacaksa, kaÃ§ dondurma hazÄ±rlamalÄ±?

| SÄ±caklÄ±k (Â°C) | SatÄ±ÅŸ (Adet) |
|---------------|--------------|
| 20            | 30           |
| 22            | 35           |
| 25            | 45           |
| 28            | 55           |
| 30            | 60           |
| 32            | 70           |
| 35            | 80           |

**Soru:** 35Â°C'de kaÃ§ dondurma satÄ±lÄ±r?

---

## ğŸ“Š Matematiksel Temel

### En KÃ¼Ã§Ã¼k Kareler YÃ¶ntemi (Ordinary Least Squares - OLS)

Linear regresyon, **hata karelerinin toplamÄ±nÄ± minimize eden** en iyi doÄŸruyu bulur.

#### EÄŸim (m) Hesaplama

```
m = Î£[(xi - xÌ„)(yi - È³)] / Î£[(xi - xÌ„)Â²]
```

#### Kesim (b) Hesaplama

```
b = È³ - m Ã— xÌ„
```

Burada:
- `xÌ„` = Ortalama x deÄŸeri
- `È³` = Ortalama y deÄŸeri
- `Î£` = Toplam sembolÃ¼

### ğŸ§® AdÄ±m AdÄ±m Hesaplama

```python
import numpy as np

# Veri
sicaklik = np.array([20, 22, 25, 28, 30, 32, 35])
satis = np.array([30, 35, 45, 55, 60, 70, 80])

# Ortalamalar
x_mean = np.mean(sicaklik)  # 27.43
y_mean = np.mean(satis)      # 53.57

# EÄŸim (m)
numerator = np.sum((sicaklik - x_mean) * (satis - y_mean))
denominator = np.sum((sicaklik - x_mean) ** 2)
m = numerator / denominator  # 3.42

# Kesim (b)
b = y_mean - m * x_mean      # -40.14

print(f"Denklem: y = {m:.2f}x + {b:.2f}")
# Output: y = 3.42x + -40.14
```

**SonuÃ§:** Her 1Â°C artÄ±ÅŸta satÄ±ÅŸ ~3.42 adet artÄ±yor!

---

## ğŸ’» Kod Ã–rnekleri

### ğŸ”° BaÅŸlangÄ±Ã§: Manuel Uygulama

```python
import numpy as np
import matplotlib.pyplot as plt

class LinearRegression:
    """Manuel Linear Regresyon SÄ±nÄ±fÄ±"""
    
    def __init__(self):
        self.m = None  # EÄŸim
        self.b = None  # Kesim
    
    def fit(self, X, y):
        """Modeli eÄŸit"""
        x_mean = np.mean(X)
        y_mean = np.mean(y)
        
        # EÄŸim hesapla
        numerator = np.sum((X - x_mean) * (y - y_mean))
        denominator = np.sum((X - x_mean) ** 2)
        self.m = numerator / denominator
        
        # Kesim hesapla
        self.b = y_mean - self.m * x_mean
        
        return self
    
    def predict(self, X):
        """Tahmin yap"""
        return self.m * X + self.b
    
    def score(self, X, y):
        """RÂ² skorunu hesapla"""
        y_pred = self.predict(X)
        ss_tot = np.sum((y - np.mean(y)) ** 2)
        ss_res = np.sum((y - y_pred) ** 2)
        return 1 - (ss_res / ss_tot)

# KullanÄ±m
model = LinearRegression()
model.fit(sicaklik, satis)

print(f"EÄŸim: {model.m:.4f}")
print(f"Kesim: {model.b:.4f}")
print(f"RÂ² Skoru: {model.score(sicaklik, satis):.4f}")

# Tahmin
yarin_sicaklik = 35
tahmin = model.predict(yarin_sicaklik)
print(f"35Â°C'de tahmini satÄ±ÅŸ: {tahmin:.0f} dondurma")
```

### ğŸš€ Profesyonel: Sklearn ile Uygulama

```python
from sklearn.linear_model import LinearRegression
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error, r2_score

# Veriyi hazÄ±rla
X = sicaklik.reshape(-1, 1)  # 2D array'e Ã§evir
y = satis

# Train-Test split
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42
)

# Model oluÅŸtur ve eÄŸit
model = LinearRegression()
model.fit(X_train, y_train)

# Tahmin
y_pred = model.predict(X_test)

# Performans
r2 = r2_score(y_test, y_pred)
rmse = np.sqrt(mean_squared_error(y_test, y_pred))

print(f"RÂ² Score: {r2:.4f}")
print(f"RMSE: {rmse:.2f}")
print(f"EÄŸim: {model.coef_[0]:.4f}")
print(f"Kesim: {model.intercept_:.4f}")
```

### ğŸ“ˆ Ã‡ok DeÄŸiÅŸkenli Linear Regresyon

```python
import pandas as pd

# Veri (sÄ±caklÄ±k, nem, rÃ¼zgar â†’ satÄ±ÅŸ)
data = pd.DataFrame({
    'sicaklik': [20, 22, 25, 28, 30, 32, 35],
    'nem': [60, 55, 50, 45, 40, 35, 30],
    'ruzgar': [10, 15, 12, 8, 5, 7, 3],
    'satis': [30, 35, 45, 55, 60, 70, 80]
})

# Ã–zellikler ve hedef
X = data[['sicaklik', 'nem', 'ruzgar']]
y = data['satis']

# Model
model = LinearRegression()
model.fit(X, y)

print("KatsayÄ±lar:")
for feature, coef in zip(X.columns, model.coef_):
    print(f"  {feature}: {coef:.2f}")
print(f"Kesim: {model.intercept_:.2f}")

# Tahmin
yeni_veri = pd.DataFrame({
    'sicaklik': [35],
    'nem': [30],
    'ruzgar': [5]
})
tahmin = model.predict(yeni_veri)
print(f"\nTahmin: {tahmin[0]:.0f} dondurma")
```

---

## ğŸ“‰ GÃ¶rselleÅŸtirme

### Basit Scatter Plot ve Regresyon Ã‡izgisi

```python
import matplotlib.pyplot as plt
import seaborn as sns

# Stil ayarlarÄ±
sns.set_style("whitegrid")
plt.figure(figsize=(10, 6))

# Scatter plot
plt.scatter(sicaklik, satis, color='blue', s=100, 
            alpha=0.6, edgecolors='black', linewidth=2,
            label='GerÃ§ek Veriler')

# Regresyon Ã§izgisi
X_line = np.linspace(sicaklik.min(), sicaklik.max(), 100).reshape(-1, 1)
y_line = model.predict(X_line)
plt.plot(X_line, y_line, color='red', linewidth=3, 
         label='Regresyon Ã‡izgisi')

# Hata Ã§izgileri
for x, y_true in zip(sicaklik, satis):
    y_pred = model.predict([[x]])[0]
    plt.plot([x, x], [y_true, y_pred], 'g--', alpha=0.5, linewidth=1)

plt.xlabel('SÄ±caklÄ±k (Â°C)', fontsize=12, fontweight='bold')
plt.ylabel('SatÄ±ÅŸ (Adet)', fontsize=12, fontweight='bold')
plt.title('ğŸ¦ Dondurma SatÄ±ÅŸ Tahmini - Linear Regresyon', 
          fontsize=14, fontweight='bold')
plt.legend(fontsize=10)
plt.grid(True, alpha=0.3)
plt.tight_layout()
plt.show()
```

### Residual Plot (Hata Analizi)

```python
# Tahminler
y_pred = model.predict(X)
residuals = y - y_pred

# Residual plot
plt.figure(figsize=(10, 5))

plt.subplot(1, 2, 1)
plt.scatter(y_pred, residuals, color='green', s=100, alpha=0.6)
plt.axhline(y=0, color='red', linestyle='--', linewidth=2)
plt.xlabel('Tahmin Edilen DeÄŸer', fontweight='bold')
plt.ylabel('Hata (Residual)', fontweight='bold')
plt.title('Residual Plot', fontweight='bold')
plt.grid(True, alpha=0.3)

plt.subplot(1, 2, 2)
plt.scatter(y, y_pred, color='purple', s=100, alpha=0.6)
perfect_line = np.linspace(y.min(), y.max(), 100)
plt.plot(perfect_line, perfect_line, 'r--', linewidth=2, 
         label='MÃ¼kemmel Tahmin')
plt.xlabel('GerÃ§ek DeÄŸer', fontweight='bold')
plt.ylabel('Tahmin', fontweight='bold')
plt.title('GerÃ§ek vs Tahmin', fontweight='bold')
plt.legend()
plt.grid(True, alpha=0.3)

plt.tight_layout()
plt.show()
```

---

## ğŸ¯ Performans Metrikleri

### 1. RÂ² (R-Squared) - Determinasyon KatsayÄ±sÄ±

**Ne Ã¶lÃ§er?** Modelin veriyi ne kadar iyi aÃ§Ä±kladÄ±ÄŸÄ±

```python
r2 = r2_score(y_true, y_pred)
```

**Yorumlama:**
- **RÂ² = 1.0**: MÃ¼kemmel model (tÃ¼m varyansÄ± aÃ§Ä±klÄ±yor)
- **RÂ² = 0.8-0.9**: Ã‡ok iyi
- **RÂ² = 0.6-0.7**: Ä°yi
- **RÂ² < 0.5**: ZayÄ±f
- **RÂ² < 0**: Modeliniz ortalamanÄ±n altÄ±nda!

### 2. MSE (Mean Squared Error)

**Ne Ã¶lÃ§er?** HatalarÄ±n karelerinin ortalamasÄ±

```python
mse = mean_squared_error(y_true, y_pred)
```

**Ã–zellikler:**
- BÃ¼yÃ¼k hatalarÄ± daha Ã§ok cezalandÄ±rÄ±r
- Asla negatif olamaz
- Orijinal birimle aynÄ± deÄŸil (kare)

### 3. RMSE (Root Mean Squared Error)

**Ne Ã¶lÃ§er?** MSE'nin karekÃ¶kÃ¼ (orijinal birimle)

```python
rmse = np.sqrt(mean_squared_error(y_true, y_pred))
```

**AvantajlarÄ±:**
- Orijinal birimle Ã§alÄ±ÅŸÄ±r (yorumlamasÄ± kolay)
- Outlier'lara duyarlÄ±

### 4. MAE (Mean Absolute Error)

**Ne Ã¶lÃ§er?** HatalarÄ±n mutlak deÄŸerlerinin ortalamasÄ±

```python
from sklearn.metrics import mean_absolute_error
mae = mean_absolute_error(y_true, y_pred)
```

**AvantajlarÄ±:**
- Outlier'lara daha az duyarlÄ±
- Basit yorumlama

### ğŸ“Š KarÅŸÄ±laÅŸtÄ±rma

```python
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score

def evaluate_model(y_true, y_pred):
    """Model performansÄ±nÄ± deÄŸerlendir"""
    metrics = {
        'RÂ²': r2_score(y_true, y_pred),
        'MSE': mean_squared_error(y_true, y_pred),
        'RMSE': np.sqrt(mean_squared_error(y_true, y_pred)),
        'MAE': mean_absolute_error(y_true, y_pred)
    }
    
    print("ğŸ“Š Model PerformansÄ±:")
    print("-" * 40)
    for metric, value in metrics.items():
        print(f"{metric:10s}: {value:.4f}")
    
    return metrics

# KullanÄ±m
metrics = evaluate_model(y_test, y_pred)
```

---

## ğŸŒ GerÃ§ek DÃ¼nya UygulamalarÄ±

### 1. ğŸ  Ev Fiyat Tahmini

```python
# Ã–rnek veri
from sklearn.datasets import fetch_california_housing

data = fetch_california_housing()
X = data.data
y = data.target

# Model
model = LinearRegression()
model.fit(X, y)

# En Ã¶nemli Ã¶zellikler
feature_importance = pd.DataFrame({
    'Feature': data.feature_names,
    'Coefficient': model.coef_
}).sort_values('Coefficient', key=abs, ascending=False)

print(feature_importance)
```

### 2. ğŸ“ˆ SatÄ±ÅŸ Tahmini

```python
# Reklam harcamasÄ± â†’ SatÄ±ÅŸ tahmini
advertising = pd.DataFrame({
    'TV': [230, 44, 17, 151, 180],
    'Radio': [37, 39, 45, 41, 10],
    'Newspaper': [69, 45, 69, 58, 58],
    'Sales': [22, 10, 9, 18, 12]
})

X = advertising[['TV', 'Radio', 'Newspaper']]
y = advertising['Sales']

model = LinearRegression()
model.fit(X, y)

# Yeni kampanya tahmini
new_campaign = [[200, 40, 60]]
predicted_sales = model.predict(new_campaign)
print(f"Tahmini satÄ±ÅŸ: {predicted_sales[0]:.2f} bin $")
```

### 3. ğŸ“ Ã–ÄŸrenci Not Tahmini

```python
# Ã‡alÄ±ÅŸma saati â†’ SÄ±nav notu
study_hours = np.array([1, 2, 3, 4, 5, 6, 7, 8]).reshape(-1, 1)
exam_scores = np.array([45, 50, 55, 65, 70, 80, 85, 90])

model = LinearRegression()
model.fit(study_hours, exam_scores)

# 9 saat Ã§alÄ±ÅŸan Ã¶ÄŸrencinin tahmini notu
predicted_score = model.predict([[9]])
print(f"9 saat Ã§alÄ±ÅŸma â†’ Tahmini not: {predicted_score[0]:.1f}")
```

---

## ğŸš€ Ä°leri Seviye Konular

### 1. Polynomial Regression (Polinom Regresyon)

DoÄŸrusal olmayan iliÅŸkiler iÃ§in:

```python
from sklearn.preprocessing import PolynomialFeatures

# Polinom Ã¶zellikleri oluÅŸtur (derece=2)
poly = PolynomialFeatures(degree=2)
X_poly = poly.fit_transform(X)

# Model
model = LinearRegression()
model.fit(X_poly, y)

# xÂ² ve xy terimleri otomatik eklendi!
```

### 2. Regularization (Ridge & Lasso)

Overfitting'i Ã¶nlemek iÃ§in:

```python
from sklearn.linear_model import Ridge, Lasso

# Ridge (L2 Regularization)
ridge = Ridge(alpha=1.0)
ridge.fit(X_train, y_train)

# Lasso (L1 Regularization)
lasso = Lasso(alpha=0.1)
lasso.fit(X_train, y_train)
```

### 3. Feature Scaling

Ã–zellikleri normalize etme:

```python
from sklearn.preprocessing import StandardScaler

scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

model = LinearRegression()
model.fit(X_scaled, y)
```

### 4. Cross-Validation

Modeli gÃ¼venilir ÅŸekilde deÄŸerlendirme:

```python
from sklearn.model_selection import cross_val_score

scores = cross_val_score(model, X, y, cv=5, scoring='r2')
print(f"Ortalama RÂ²: {scores.mean():.4f} (Â±{scores.std():.4f})")
```

---


### ğŸŒ Online Kaynaklar
- [Scikit-learn DokÃ¼mantasyonu](https://scikit-learn.org/)
- [Khan Academy - Statistics](https://www.khanacademy.org/math/statistics-probability)
- [StatQuest YouTube](https://www.youtube.com/c/joshstarmer)


## ğŸ¤ KatkÄ±da Bulunma

Bu proje aÃ§Ä±k kaynak kodludur. KatkÄ±larÄ±nÄ±zÄ± bekliyoruz!

```bash
# Repo'yu fork edin
git clone https://github.com/bukettoptas/linear-regression-tutorial.git

# Branch oluÅŸturun
git checkout -b feature/yeni-Ã¶zellik

# Commit yapÄ±n
git commit -m "Yeni Ã¶zellik eklendi"

# Push edin
git push origin feature/yeni-Ã¶zellik

# Pull request aÃ§Ä±n
```

---


**Buket ToptaÅŸ**  
ğŸ’¼ LinkedIn: [linkedin.com/in/bukettoptas]  
ğŸ™ GitHub: [@bukettoptas](https://github.com/bukettoptas)

---

<div align="center">

**â­ Bu repo iÅŸinize yaradÄ±ysa yÄ±ldÄ±z vermeyi unutmayÄ±n! â­**

Made with â¤ï¸ for Machine Learning Students

</div>
